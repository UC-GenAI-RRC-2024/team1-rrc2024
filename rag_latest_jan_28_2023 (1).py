# -*- coding: utf-8 -*-
"""RAG_latest_Jan 28-2023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eQRxbI2lF-RoT5CtFFHEnHV3lX0_txSs
"""

# Install necessary packages using pip
!pip install langchain                # Install the langchain package
!pip install -U langchain-openai      # Upgrade or install the langchain-openai package
!pip install nltk                     # Install the NLTK package for natural language processing
!pip install chromadb                 # Install the chromadb package for language data storage

# Import required Python libraries and modules
import os                            # Operating system module
import pandas as pd                  # Data manipulation library
from langchain_openai import OpenAIEmbeddings  # Module for OpenAI embeddings
from langchain.vectorstores import Chroma      # Module for Chroma DB
from langchain.llms import OpenAI              # Module for using OpenAI's language models
from langchain.chains import RetrievalQA        # Module for setting up the RAG pipeline
from langchain.retrievers import SelfQueryRetriever  # Module for retrieval using self-query
from dotenv import load_dotenv         # Module for loading environment variables
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Module for text splitting
import nltk                           # Natural Language Toolkit
nltk.download('punkt')                # Download the NLTK tokenizer data for sentence tokenization
from nltk.tokenize import sent_tokenize # Import sentence tokenization function from NLTK
import numpy as np                    # Numerical computing library

# Hardcoded API Key (not recommended)
openai_api_key = 'your-openai-api-key'  # Replace 'your-openai-api-key' with your actual OpenAI API key

# Set the API key for the session
os.environ["OPENAI_API_KEY"] = openai_api_key  # Sets the OpenAI API key in the environment variable

llm_name = "gpt-3.5-turbo"  # Assigns the string "gpt-3.5-turbo" to the variable 'llm_name'
print(llm_name)            # Prints the value of 'llm_name' to the console

# Import the ChatOpenAI class from the langchain.chat_models module
from langchain.chat_models import ChatOpenAI

# Create an instance of the ChatOpenAI class
llm = ChatOpenAI(model_name=llm_name, temperature=1)

import pandas as pd  # Import the pandas library for data manipulation
import io
from IPython.display import display
import ipywidgets as widgets  # Import ipywidgets for creating interactive widgets in IPython
import os

# Function to concatenate CSVs from uploaded file contents
def concatenate_csvs(csv_file_contents):
    df_list = []
    for file_content in csv_file_contents:
        content = io.StringIO(file_content.decode('utf-8'))
        df = pd.read_csv(content)
        df_list.append(df)
    concatenated_df = pd.concat(df_list, ignore_index=True)
    return concatenated_df

# Function to format DataFrame to a string
def format_df_to_txt(df):
    formatted_string = df.apply(lambda x: ' | '.join(x.astype(str)), axis=1).str.cat(sep='\n')
    return formatted_string

# Function to save a string to a text file
def save_string_to_txt(data_string, output_path):
    with open(output_path, 'w') as file:
        file.write(data_string)

# Function to process data when submit button is clicked
def on_submit_clicked(b):
    if file_input.value:
        csv_contents = [v['content'] for v in file_input.value.values()]
        concatenated_df = concatenate_csvs(csv_contents)
        formatted_string = format_df_to_txt(concatenated_df)
        output_txt_path = "formatted_data.txt"
        save_string_to_txt(formatted_string, output_txt_path)
        print("Data processed and saved to 'formatted_data.txt'.")
        display_formatted_data(output_txt_path)
    else:
        print("No files uploaded.")

# Function to reset file upload widget and delete the created file
def on_reset_clicked(b):
    global output_txt_path  # Declare output_txt_path as a global variable
    file_input.value.clear()
    file_input._counter = 0
    print("File selection reset.")

    # Delete the created file if it exists
    if output_txt_path and os.path.exists(output_txt_path):
        os.remove(output_txt_path)
        print("Deleted the created file '{}'.".format(output_txt_path))
        output_txt_path = None  # Set the global variable to None

# File upload widget
file_input = widgets.FileUpload(accept='.csv', multiple=True)

# Submit button
submit_button = widgets.Button(description="Upload & Process Files")
submit_button.on_click(on_submit_clicked)

# Reset button
reset_button = widgets.Button(description="Reset File Selection & Delete File")
reset_button.on_click(on_reset_clicked)

# Display widgets
display(widgets.HBox([file_input, submit_button, reset_button]))

# Function to read and display the first few lines of the formatted text file
def display_formatted_data(file_path, num_lines=10):
    with open(file_path, 'r') as file:
        lines = file.readlines()
        print("Displaying first {} lines of formatted data:".format(num_lines))
        for line in lines[:num_lines]:
            print(line)

output_txt_path = None  # Initialize output_txt_path as None

import os
import shutil
import pandas as pd
import io
from IPython.display import display
import ipywidgets as widgets
from tqdm.notebook import tqdm  # Import tqdm for creating a progress bar

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma  # Assuming Chroma is available in langchain

# Global variables
chunks = None
vectordb = None
persist_directory = '/content/'  # Define persist_directory as a global variable

# Function to split text into chunks
def create_chunks(file_path, chunk_size, overlap_size):
    global chunks
    with open(file_path, 'r', encoding='utf-8') as file:
        document = file.read()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap_size)
    chunks = text_splitter.split_text(document)
    print(f"Number of chunks created: {len(chunks)}")

# Updated function to create embeddings using Chroma with a progress bar
def create_embeddings_with_chroma():
    global chunks, vectordb
    try:
        embeddings = OpenAIEmbeddings()

        # Initialize the progress bar
        progress_bar = tqdm(total=len(chunks), desc='Creating Embeddings')

        # Create embeddings and update the progress bar
        vectordb = Chroma.from_texts(texts=chunks, embedding=embeddings, persist_directory=persist_directory)

        for _ in chunks:
            progress_bar.update(1)  # Update progress bar by one step

        progress_bar.close()
        print(f"Number of documents in vectordb: {vectordb._collection.count()}")
    except Exception as e:
        print("An error occurred while creating embeddings:", e)
        progress_bar.close()

# Function to initiate chunk creation
def on_create_chunks_clicked(b):
    chunk_size = chunk_size_widget.value
    overlap_size = overlap_size_widget.value
    create_chunks("formatted_data.txt", chunk_size, overlap_size)

# Function to initiate embedding creation
def on_start_embedding_clicked(b):
    create_embeddings_with_chroma()

# Function to reset embeddings database
def on_reset_embedding_clicked(b):
    global vectordb
    vectordb = None
    # Delete database files if stored on disk
    if os.path.exists(persist_directory):
        shutil.rmtree(persist_directory)
    print("Embeddings database reset.")

# Widgets
chunk_size_widget = widgets.IntText(value=1000, description='Chunk Size:')
overlap_size_widget = widgets.IntText(value=150, description='Overlap Size:')
create_chunks_button = widgets.Button(description="Create Chunks")
create_chunks_button.on_click(on_create_chunks_clicked)
start_embedding_button = widgets.Button(description="Start Embedding")
start_embedding_button.on_click(on_start_embedding_clicked)
reset_embedding_button = widgets.Button(description="Reset VectorDB")
reset_embedding_button.on_click(on_reset_embedding_clicked)

# Display widgets
display(widgets.VBox([chunk_size_widget, overlap_size_widget, create_chunks_button,
                      start_embedding_button, reset_embedding_button]))

import ipywidgets as widgets  # Import widgets for creating interactive elements
from IPython.display import display, clear_output, HTML
from langchain.chains import ConversationalRetrievalChain  # Import chain for conversational retrieval
from langchain.memory import ConversationBufferMemory  # Import memory for conversation history
from langchain.chat_models import ChatOpenAI  # Import a chat model, likely defined earlier

# Initialize chatbot_chain for single document context
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
chatbot_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,  # Assuming 'llm' is defined earlier as a chat model
    retriever=vectordb.as_retriever(),  # Assuming 'vectordb' is set up for single document
    memory=memory
)

# Define a simplified prompt template
template = "Question: {message}\nDetailed answer:"

class ChatbotInterface:
    def __init__(self, chatbot_chain):
        self.chatbot_chain = chatbot_chain
        self.chat_history = []
        self.create_widgets()
        self.display_widgets()

    def create_widgets(self):
        # Create interactive widgets for user input, sending messages, history display, and output
        self.text_input = widgets.Text(placeholder='Type your message', description='Message:')
        self.send_button = widgets.Button(description='Send')
        self.send_button.on_click(self.send_message)
        self.history_button = widgets.Dropdown(options=['', 'See Conversation History'], description='Options')
        self.history_button.observe(self.display_history, names='value')
        self.close_history_button = widgets.Button(description='Close History')
        self.close_history_button.on_click(self.close_history)
        self.output_area = widgets.Output(layout={'border': '1px solid black', 'width': '45%'})
        self.response_area = widgets.Output(layout={'border': '1px solid red', 'width': '45%'})
        self.history_area = widgets.Output(layout={'border': '1px solid black', 'background_color': 'lightyellow', 'width': '60%', 'overflow': 'auto', 'max_height': '500px'})

    def display_widgets(self):
        # Display the widgets in an HBox (horizontal box)
        display(widgets.HBox([widgets.VBox([self.text_input, self.send_button, self.history_button, self.output_area, self.response_area]), self.history_area]))

    def send_message(self, b):
        combined_input = self.text_input.value
        if combined_input:
            prompt = template.format(message=combined_input)
            response = self.chatbot_chain({"question": prompt, "chat_history": self.chat_history})
            answer = response['answer']
            self.chat_history.append(('User', combined_input))
            self.chat_history.append(('Bot', answer))
            self.update_output(combined_input, answer)
            # Hide history area on new message
            with self.history_area:
                clear_output()
        self.text_input.value = ''

    def display_history(self, change):
        if change['new'] == 'See Conversation History':
            with self.history_area:
                clear_output(wait=True)
                history_html = "<div style='font-family: Arial; background-color: lightyellow;'>"
                for role, message in self.chat_history:
                    history_html += f"<b>{role}</b>: {message}<br>"
                history_html += f"<br><button onclick=\"document.getElementById('{self.close_history_button.model_id}').click();\">Close History</button></div>"
                display(HTML(history_html))
            self.history_button.value = ''  # Reset the dropdown

    def close_history(self, b):
        with self.history_area:
            clear_output()

    def update_output(self, user_message, bot_response):
        with self.response_area:
            clear_output(wait=True)
            response_html = f"<div style='font-family: Arial; background-color: white;'>{bot_response}</div>"
            display(HTML(response_html))

# Initialize the Chatbot Interface with the chatbot_chain
chat_interface = ChatbotInterface(chatbot_chain)